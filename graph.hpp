
#ifndef TG_GRAPH
#define TG_GRAPH

#include "layer.hpp"
#include <memory>

namespace tg
{

template <typename T>
class Graph : public Layer<T> {

private:
    std::vector<std::shared_ptr<Layer<T>>> m_chain;

public:
    Graph() = default;
    ~Graph() = default;

    Graph(const Graph& other) = delete;
    Graph& operator=(const Graph& other) = delete;

    Graph(Graph&& other) noexcept {
        m_chain = std::move(other.m_chain);
    }

    Graph& operator=(Graph&& other) noexcept {
        if (this == &other) return *this;
        m_chain = std::move(other.m_chain);
        return *this;
    }

    std::shared_ptr<Layer<T>> operator+(const std::shared_ptr<Layer<T>> input) {
        m_chain.push_back(input);
        return input;
    }

    const std::vector<std::shared_ptr<Tensor<T>>> forward(const std::vector<std::shared_ptr<Tensor<T>>>& inputs) override {
        auto result = m_chain[0]->forward(inputs);
        for (uint32_t i = 1u; i < m_chain.size(); ++i) {
            result = m_chain[i]->forward(result);
        }
        return result;
    }

    const std::vector<std::shared_ptr<Tensor<T>>> fit(const std::vector<std::shared_ptr<Tensor<T>>>& inputs, const std::vector<std::shared_ptr<Tensor<T>>>& labels, std::string loss) {
        static_assert(std::is_floating_point<T>::value, "Only floats are supported");
        if ((inputs.size() != 1u) || (labels.size() != 1u)) throw std::invalid_argument("Just one Tensor in/out is supported");
        std::vector<std::shared_ptr<Tensor<T>>> result;
        std::vector<std::shared_ptr<Tensor<T>>> backInput;
        auto predict = forward(inputs);
        result.push_back(std::make_shared<Tensor<T>>(std::move(predict[0]->copy()))); // output
        if ("binary_crossentropy" == loss) {
            // Loss function for logistic regression:
            // Loss = -(Y / predict) + ((1-Y)/(1-predict))
            // Loss with "True" lables
            auto dLoss_t = *(labels[0].get()) / *(predict[0].get()); // (Y / predict)
            dLoss_t *= (-1.f); // -(Y / predict)
            // Loss with "False" lables
            auto dLoss_f = *(labels[0].get()) * (-1.f); // -Y
            dLoss_f += 1.f; // 1-Y
            *(predict[0].get()) *= (-1.f); // -predict
            *(predict[0].get()) += 1.f; // 1-predict
            dLoss_f /= *(predict[0].get()); // (1-Y)/(1-predict)
            dLoss_t += dLoss_f; // -(Y / predict) + ((1-Y)/(1-predict))
            auto dLoss = std::make_shared<Tensor<T>>(std::move(dLoss_t));
            backInput.push_back(dLoss);
        } else {
            throw std::invalid_argument("Unsupported loss function");
        }
        backward(backInput);
        return result;
    }

    const std::vector<std::shared_ptr<Tensor<T>>> backward(const std::vector<std::shared_ptr<Tensor<T>>>& grads) override {
        auto result = m_chain.rbegin()->get()->backward(grads);
        for (auto itr = m_chain.rbegin()+1u; itr != m_chain.rend(); ++itr) {
            result = itr->get()->backward(result);
        }
        return result;
    }

    void set(std::string name, const std::shared_ptr<Tensor<T>>& val) override {
        throw std::runtime_error("Unsupported functionality");
    }

    std::vector<size_t>& shape() override {
        throw std::runtime_error("Unsupported functionality");
    }
    void info() override {
        std::cout << "===============Graph===============" << std::endl;
        std::cout << "|--> " << m_chain.size() << " layers" << std::endl;
        for (auto layer : m_chain) {
            layer->info();
        }
    }

    void reconstructionToStream(std::ostream& os, std::string graphId, std::string layerId, std::string prevLayerId) override {
        os << "/** Autogenerated header to reconstruct graph */" << std::endl;
        os << "\n#pragma once" << std::endl;
        os << "\n#include \"graph.hpp\"" << std::endl;
        os << "\n#include \"input.hpp\"" << std::endl;
        os << "\n#include \"dense.hpp\"" << std::endl;
        os << "\ntg::Graph<" << this->typeToString() << "> reconstruct_" << graphId << "(void) {" << std::endl;
        os << "    auto " << graphId << " = tg::Graph<" << this->typeToString() << ">();" << std::endl;
        size_t layerIdx = 0;
        std::string prevId;
        std::string curId;
        for (auto layer : m_chain) {
            curId = graphId + "_lr" + std::to_string(layerIdx++);
            layer->reconstructionToStream(os, graphId, curId, prevId);
            prevId = curId;
        }
        os << "    return " << graphId << ";" << std::endl;
        os << "\n}" << std::endl;
    }

};
} // namespace tg
#endif // TG_GRAPH